{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed libraries\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from polars import col\n",
    "import yfinance as yf\n",
    "from ta import add_all_ta_features\n",
    "from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday, nearest_workday, USMartinLutherKingJr, USPresidentsDay, GoodFriday, USMemorialDay, USLaborDay, USThanksgivingDay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_stats(df, cols):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame and a list of column names and calculates the moving average, maximum and minimum values for each column over a list of periods. \n",
    "    The results are added to the DataFrame as new columns.\n",
    "\n",
    "    :param df: DataFrame containing data to calculate moving statistics on\n",
    "    :type df: pandas.DataFrame\n",
    "    :param cols: List of column names to calculate moving statistics on\n",
    "    :type cols: list[str]\n",
    "    :return: Modified DataFrame with new columns containing moving statistics\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a list of periods to calculate\n",
    "    periods = [2, 7, 15, 30]\n",
    "    \n",
    "    # Loop through each column in the list\n",
    "    for col in cols:\n",
    "        df[f'PWD {col}'] = df[col].shift(1)\n",
    "        # Loop through each period in the list\n",
    "        for p in periods:\n",
    "            # Calculate moving average using rolling() and mean()\n",
    "            df[f\"{col}_MA_{p}\"] = df[col].rolling(p).mean()\n",
    "            # Calculate moving max using rolling() and max()\n",
    "            df[f\"{col}_MAX_{p}\"] = df[col].rolling(p).max()\n",
    "            # Calculate moving min using rolling() and min()\n",
    "            df[f\"{col}_MIN_{p}\"] = df[col].rolling(p).min()\n",
    "\n",
    "    # Remove current day's input columns\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "    return df # Return the modified dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that slices and casts a column\n",
    "def slice_and_cast(col):\n",
    "    \"\"\"\n",
    "    Slices and casts a column to a Date type.\n",
    "\n",
    "    This function takes a column name `col` as input and returns an expression that can be used to slice the first 10 characters from the column, parse them as a date in the format \"%Y-%m-%d\", and cast the result to a Date type.\n",
    "\n",
    "    The function uses the `str.slice`, `str.strptime`, and `cast` methods from the polars library to perform these operations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col : str\n",
    "        The name of the column to slice and cast.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.Expr\n",
    "        An expression that can be used to slice and cast the specified column.\n",
    "    \"\"\"\n",
    "    \n",
    "    return pl.col(col).str.slice(0, 10).str.strptime(pl.Datetime, \"%Y-%m-%d\").cast(pl.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_column(df):\n",
    "    \"\"\"\n",
    "    Splits the 'File' column of a DataFrame into multiple columns.\n",
    "\n",
    "    This function takes a DataFrame `df` as input and returns a new DataFrame where the 'File' column has been split into multiple columns: 'Query_Type', 'Query', and 'FileName'. The original 'File' column is retained in the resulting DataFrame.\n",
    "\n",
    "    The function first slices characters 22 to 30 from the 'File' column using the `str.slice` method. Then it applies a lambda function to split the 'File' column on '/' and creates new columns for each part of the split using the `apply` and `alias` methods.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The input DataFrame with a 'File' column to be split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        A new DataFrame with the 'File' column split into multiple columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.with_columns(pl.col(\"File\").str.slice(22, 30))\n",
    "    df = df.with_columns([\n",
    "                           col('File'),\n",
    "                           *[col('File').apply(lambda s, i=i: s.split('/')[i]).alias(col_name)\n",
    "                            for i, col_name in enumerate(['Query_Type', 'Query', 'FileName'])]\n",
    "                        ])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_sentiment(df):\n",
    "   \"\"\"\n",
    "   This function takes in a DataFrame and returns a new DataFrame with aggregated sentiment and signal data.\n",
    "\n",
    "   The input DataFrame should have columns 'Query_Type', 'Sentiment_Label', 'Signal_Label', and 'Created_at'.\n",
    "   The function groups the data by 'Created_at', 'Query', and 'Query_Type' and calculates the sum of positive and negative sentiments,\n",
    "   bullish and bearish signals, as well as the total daily tweets for each group.\n",
    "\n",
    "   :param df: A DataFrame with columns 'Query_Type', 'Sentiment_Label', 'Signal_Label', and 'Created_at'\n",
    "   :return: A new DataFrame with aggregated sentiment and signal data\n",
    "   \"\"\"\n",
    "   Query_Type=df[\"Query_Type\"][0]\n",
    "   features_querry = (df.lazy().groupby(\n",
    "       [\"Created_at\", \"Query\", \"Query_Type\"]).agg([\n",
    "           (pl.col('Sentiment_Label') == \"Positive\"\n",
    "            ).sum().alias(f'{Query_Type} Sentiment Positive'),\n",
    "           (pl.col('Sentiment_Label') == \"Negative\"\n",
    "            ).sum().alias(f'{Query_Type} Sentiment Negative'),\n",
    "           (pl.col('Signal_Label') == \"Bullish\"\n",
    "            ).sum().alias(f'{Query_Type} Signal Bullish'),\n",
    "           (pl.col('Signal_Label') == \"Bearish\"\n",
    "            ).sum().alias(f'{Query_Type} Signal Bearish'),\n",
    "           (pl.col('Created_at')).count().alias(f'{Query_Type} Total Daily Tweets'),\n",
    "       ]).sort(\"Created_at\", descending=False))\n",
    "\n",
    "   return features_querry.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_company_column(df):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame and returns a new DataFrame with an added 'Company' column.\n",
    "\n",
    "    The input DataFrame should have a 'Query' column with values 'AMZN', 'AAPL', 'MSFT', 'Jeff Bezos', 'Tim Cook', 'Elon Musk' or 'Satya Nadella'.\n",
    "    The function maps these values to their respective company names: 'AMAZON', 'APPLE', 'TSLA' and 'MICROSOFT'.\n",
    "    If the value in the 'Query' column is not one of these six, the value in the new 'Company' column will be set to 'ERROR'.\n",
    "\n",
    "    :param df: A DataFrame with a 'Query' column\n",
    "    :return: A new DataFrame with an added 'Company' column\n",
    "    \"\"\"\n",
    "    return df.with_columns(\n",
    "        pl.when((pl.col(\"Query\") == \"AMZN\") | (pl.col(\"Query\") == \"Jeff Bezos\"))\n",
    "        .then(pl.lit(\"AMAZON\"))\n",
    "        .when((pl.col(\"Query\") == \"AAPL\") | (pl.col(\"Query\") == \"Tim Cook\"))\n",
    "        .then(pl.lit(\"APPLE\"))\n",
    "        .when((pl.col(\"Query\") == \"MSFT\") | (pl.col(\"Query\") == \"Satya Nadella\"))\n",
    "        .then(pl.lit(\"MICROSOFT\"))\n",
    "        .when((pl.col(\"Query\") == \"TSLA\") | (pl.col(\"Query\") == \"Elon Musk\"))\n",
    "        .then(pl.lit(\"TESLA\"))\n",
    "        .otherwise(pl.lit(\"ERROR\"))\n",
    "        .alias(\"Company\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_columns(df, keywords):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame and a list of keywords and returns a list of column names that contain any of the specified keywords.\n",
    "\n",
    "    :param df: A DataFrame\n",
    "    :param keywords: A list of strings representing the keywords to search for in the column names\n",
    "    :return: A list of column names that contain any of the specified keywords\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    for col in df.columns:\n",
    "        if any(keyword in col for keyword in keywords):\n",
    "            columns.append(col)\n",
    "    return columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TS Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_columns(df):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame and shifts all columns except \"Company\" down by one row. The shifted columns are renamed with a 'PWD' prefix.\n",
    "    The original columns except for 'Open', 'Close', 'Movement', 'Price Change' are then droped.\n",
    "\n",
    "    :param df: DataFrame containing data to shift\n",
    "    :type df: pandas.DataFrame\n",
    "    :return: Modified DataFrame with shifted columns\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the input DataFrame to avoid modifying it directly\n",
    "    df = df.copy()\n",
    "\n",
    "    # Loop through each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "       if col !=\"Company\":\n",
    "        # Shift the column down by one row and rename it with a 'PWD' prefix\n",
    "         df[f'PWD {col}'] = df[col].shift(1)\n",
    "         # Skip the specified columns\n",
    "         if col not in ['Open', 'Close','Adj Close', 'Movement', 'Price Change']:\n",
    "            # Drop the original column from the DataFrame\n",
    "            df = df.drop(col, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_dates(df):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe with a DatetimeIndex and checks if any dates are missing. If a date is missing, \n",
    "    the function adds a new row to the dataframe with the same values as the previous row.\n",
    "\n",
    "    :param df: The input dataframe.\n",
    "    :type df: pd.DataFrame\n",
    "    :return: A new dataframe with missing dates filled.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a date range that covers all trading days between the start and end dates of the input dataframe\n",
    "    all_dates = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    \n",
    "    # Reindex the input dataframe using this date range\n",
    "    df = df.reindex(all_dates)\n",
    "    \n",
    "    # Forward fill any missing values (i.e. copy values from previous row)\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_trading_days(df):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame with a DatetimeIndex and adds a new column 'Is Trading Day' that indicates whether each date is a trading day or not. A trading day is defined as a weekday that is not a holiday according to the USTradingCalendar.\n",
    "\n",
    "    :param df: DataFrame with a DatetimeIndex\n",
    "    :type df: pandas.DataFrame\n",
    "    :return: Modified DataFrame with new 'Is Trading Day' column\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an instance of the USTradingCalendar class\n",
    "    cal = USTradingCalendar()\n",
    "    \n",
    "    # Get all holidays between the start and end dates of the input DataFrame's index\n",
    "    holidays = cal.holidays(start=df.index.min(), end=df.index.max())\n",
    "    \n",
    "    # Create a boolean mask indicating whether each date in the index is a holiday or falls on a weekend (Saturday or Sunday)\n",
    "    mask = df.index.isin(holidays) | (df.index.dayofweek == 5) | (df.index.dayofweek == 6)\n",
    "    \n",
    "    # Invert the mask and convert it to an integer array (1 for trading days, 0 for non-trading days)\n",
    "    df['Is Trading Day'] = (~mask).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(company: str, ticker: str):\n",
    "    \"\"\"\n",
    "    This function takes in a company name and ticker symbol as arguments and returns a dataframe with stock data and technical analysis features.\n",
    "\n",
    "    :param company: The name of the company (e.g. \"APPLE\").\n",
    "    :type company: str\n",
    "    :param ticker: The ticker symbol for the company (e.g. \"AAPL\").\n",
    "    :type ticker: str\n",
    "    :return: A dataframe containing stock data and technical analysis features for the specified company.\n",
    "    :rtype: pd.DataFrame\n",
    "\n",
    "    The returned dataframe contains the following columns:\n",
    "        - Company: The name of the company.\n",
    "        - Price Change: The change in adjusted closing price from the previous day.\n",
    "        - Movement: Whether the price went \"Up\" or \"Down\" from the previous day.\n",
    "\n",
    "    In addition to these columns, the dataframe also contains several technical analysis features calculated using \n",
    "    the `add_all_ta_features` function from the `ta` library. \n",
    "    \"\"\"\n",
    "    \n",
    "    df = yf.download(ticker,\n",
    "                     start=\"2018-01-01\",\n",
    "                     end=\"2021-12-31\")\n",
    "    \n",
    "    df[\"Company\"] = company\n",
    "    \n",
    "    df[\"Price Change\"] = df[\"Adj Close\"] - df[\"Adj Close\"].shift(1)\n",
    "   \n",
    "    # Apply a function to assign Up or Down based on Price Change\n",
    "    df[\"Movement\"] = df[\"Price Change\"].apply(lambda x: \"Up\" if x > 0 else \"Down\")\n",
    "   \n",
    "     # Add all technical analysis features\n",
    "    df = add_all_ta_features(\n",
    "         df, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\")\n",
    "     \n",
    "    # Fill missing dates by copying values from previous row\n",
    "    df = fill_missing_dates(df)\n",
    "     \n",
    "    # Shift columns one day behind to avoid leakage \n",
    "    df = shift_columns(df)\n",
    "     \n",
    "    # Add trading days\n",
    "    df = map_trading_days(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USTradingCalendar(AbstractHolidayCalendar):\n",
    "    \"\"\"\n",
    "    This class defines a custom trading calendar for the United States. It inherits from the AbstractHolidayCalendar class and specifies a list of holiday rules.\n",
    "\n",
    "    The holiday rules include New Year's Day, Martin Luther King Jr. Day, Presidents' Day, Good Friday, Memorial Day, Independence Day, Labor Day, Thanksgiving Day and Christmas. The observance of holidays that fall on a weekend is determined by the nearest_workday function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a list of holiday rules\n",
    "    rules = [\n",
    "        Holiday('NewYearsDay', month=1, day=1, observance=nearest_workday),\n",
    "        USMartinLutherKingJr,\n",
    "        USPresidentsDay,\n",
    "        GoodFriday,\n",
    "        USMemorialDay,\n",
    "        Holiday('USIndependenceDay', month=7, day=4, observance=nearest_workday),\n",
    "        USLaborDay,\n",
    "        USThanksgivingDay,\n",
    "        Holiday('Christmas', month=12, day=25, observance=nearest_workday)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_features(df, date_column):\n",
    "    \"\"\"\n",
    "    Extracts day of week, day of month, day of year, quarter, week of year, and day of quarter features from a date column of a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    date_column (str): The name of the date column in the input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The input DataFrame with six new columns for the day of the week, day of the month, day of the year, quarter, week of year, and day of quarter.\n",
    "    \"\"\"\n",
    "    df['day_of_week'] = pd.to_datetime(df[date_column]).dt.dayofweek\n",
    "    df['day_of_month'] = pd.to_datetime(df[date_column]).dt.day\n",
    "    df['day_of_year'] = pd.to_datetime(df[date_column]).dt.dayofyear\n",
    "    df['month'] = pd.to_datetime(df[date_column]).dt.month\n",
    "    df['quarter'] = pd.to_datetime(df[date_column]).dt.quarter\n",
    "    df['year'] = pd.to_datetime(df[date_column]).dt.year\n",
    "    df['week_of_year'] = pd.to_datetime(df[date_column]).dt.weekofyear\n",
    "    df['day_of_quarter'] = df['day_of_year'] - pd.to_datetime(df[date_column]).dt.to_period('Q').dt.start_time.dt.dayofyear + 1\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tweet Datasets\n",
    "tickers_df=pl.read_csv('../Data/ScoredDf/Tickers.csv', sep='~', encoding='utf-8')\n",
    "ceos_df=pl.read_csv('../Data/ScoredDf/Ceos.csv', sep='~', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Spaces Column names and replace them with _\n",
    "tickers_df.columns = list(map(lambda x: x.replace(\" \", \"_\"), tickers_df.columns))\n",
    "ceos_df.columns = list(map(lambda x: x.replace(\" \", \"_\"), ceos_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the slice function to the date column to create the dates\n",
    "tickers_df = tickers_df.with_columns(slice_and_cast(\"Created_at\"))\n",
    "ceos_df = ceos_df.with_columns(slice_and_cast(\"Created_at\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the split column to create the query columns\n",
    "tickers_df = split_column(tickers_df)\n",
    "ceos_df = split_column(ceos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pivots per Day\n",
    "tickers_sentiment_features = pivot_sentiment(tickers_df)\n",
    "ceos_sentiment_features = pivot_sentiment(ceos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Company Columns\n",
    "tickers_sentiment_features = add_company_column(tickers_sentiment_features)\n",
    "ceos_sentiment_features = add_company_column(ceos_sentiment_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Columns for moving stats and call the function to create the features\n",
    "keywords = [\"Sentiment\", \"Signal\", \"Total Daily\"]\n",
    "\n",
    "cols_tickers = find_columns(tickers_sentiment_features, keywords)\n",
    "tickers_sentiment_features = get_moving_stats(tickers_sentiment_features.to_pandas(), cols_tickers)\n",
    "\n",
    "cols_ceos = find_columns(ceos_sentiment_features, keywords)\n",
    "ceos_sentiment_features = get_moving_stats(ceos_sentiment_features.to_pandas(), cols_ceos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([(2019-12-08 00:00:00, 'AMAZON')], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if Indexes are the same size\n",
    "idx1 = pd.Index(tickers_sentiment_features[[\"Created_at\", \"Company\"]])\n",
    "idx2 = pd.Index(ceos_sentiment_features[[\"Created_at\", \"Company\"]])\n",
    "idx1.difference(idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the two datasets\n",
    "twitter_features=tickers_sentiment_features.merge(ceos_sentiment_features, on=[\"Created_at\", \"Company\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "twitter_features = twitter_features.fillna(method=\"backfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the non Query Columns and rename the \"Created_at\" column to \"Date\"\n",
    "cols = [c for c in twitter_features.columns if \"Query\" not in c]\n",
    "twitter_features = twitter_features[cols].rename(columns={'Created_at': 'Date'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techincal Analysis Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwitterFeatures\\lib\\site-packages\\ta\\trend.py:780: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwitterFeatures\\lib\\site-packages\\ta\\trend.py:785: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwitterFeatures\\lib\\site-packages\\ta\\trend.py:780: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwitterFeatures\\lib\\site-packages\\ta\\trend.py:785: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwitterFeatures\\lib\\site-packages\\ta\\trend.py:780: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwitterFeatures\\lib\\site-packages\\ta\\trend.py:785: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwitterFeatures\\lib\\site-packages\\ta\\trend.py:780: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwitterFeatures\\lib\\site-packages\\ta\\trend.py:785: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n"
     ]
    }
   ],
   "source": [
    "#Get the stock and TS data \n",
    "aapl_ta=get_stock_data(\"APPLE\", \"AAPL\")\n",
    "msft_ta=get_stock_data(\"MICROSOFT\", \"MSFT\")\n",
    "amzn_ta=get_stock_data(\"AMAZON\", \"AMZN\")\n",
    "tsla_ta=get_stock_data(\"TESLA\", \"TSLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes and remove index and rename it to Date for merging with Tweet Features\n",
    "technical_df = pd.concat([amzn_ta, msft_ta, aapl_ta, tsla_ta]).reset_index().rename(columns={'index': 'Date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the dates after 2019-01-01\n",
    "technical_df = technical_df[technical_df['Date'] >= '2019-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimta\\AppData\\Local\\Temp\\ipykernel_25072\\827935584.py:18: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  df['week_of_year'] = pd.to_datetime(df[date_column]).dt.weekofyear\n"
     ]
    }
   ],
   "source": [
    "# Extrct Date Features\n",
    "technical_df = extract_date_features(technical_df,\"Date\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final DataFrame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge TS and Tweet Features\n",
    "final_df=technical_df.merge(twitter_features, on=[\"Date\", \"Company\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Movement Collumn to the Tweet DF\n",
    "movementDf=technical_df[[\"Date\", \"day_of_week\",\"day_of_month\",\"day_of_year\",\"month\",\"year\", \"Company\",\"Adj Close\", \"Movement\"]]\n",
    "tweet_df=twitter_features.merge(movementDf, on=[\"Date\", \"Company\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to csv to enable modeling\n",
    "tweet_df.to_csv(\"../Data/FinalDF/TweetDF.csv\",encoding=\"utf-8\",sep=\"~\", index=False)\n",
    "technical_df.to_csv(\"../Data/FinalDF/TaDF.csv\",encoding=\"utf-8\",sep=\"~\", index=False)\n",
    "final_df.to_csv(\"../Data/FinalDF/FinalDF.csv\",encoding=\"utf-8\",sep=\"~\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TwitterFeatures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07cb061406cf46499cfe37244e70ea4bcd96108fd1d0758d33c007e36c277ce0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2307c279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T18:52:49.840860Z",
     "start_time": "2022-08-09T18:52:49.789559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function transformers.utils.logging.set_verbosity_error()>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import lox\n",
    "import time\n",
    "import emoji\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from datetime import datetime\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf3c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained models paths\n",
    "STOCKWITS = '../Models/Sentiment/roberta-stocktwits-finetuned/snapshots/13_11_22/'\n",
    "ROBERTA = '../Models/Sentiment/twitter-roberta-latest/snapshots/13_11_22/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d4feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "  texts = str(text)\n",
    "  # make text lowercase\n",
    "  texts = texts.lower()\n",
    "  # remove URLs\n",
    "  texts = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", \"\", texts)\n",
    "  texts = re.sub(r'https?://\\S+', \"\", texts)\n",
    "  texts = re.sub(r'www.\\S+', \"\", texts)\n",
    "  texts = re.sub(r'&[a-z]+;', \"\", texts)\n",
    "  # Remove placeholders\n",
    "  texts = re.sub(r'{link}', \"\", texts)\n",
    "  texts = re.sub(r\"\\[video\\]\", \"\", texts)\n",
    "  # remove '\n",
    "  texts = re.sub(r'{&#39;}', \"'\", texts)\n",
    "  # remove new lines\n",
    "  texts = re.sub(r'{\\n}', \" \", texts)\n",
    "  \n",
    "  texts = texts.strip()\n",
    "  # remove symbol names\n",
    "  texts = re.sub(r'(\\#)(\\S+)', r'hashtag_\\2', texts)\n",
    "  texts = re.sub(r'(\\$)([A-Za-z]+)', r'cashtag_\\2', texts)\n",
    "  # remove usernames\n",
    "  texts = re.sub(r'(\\@)(\\S+)', r'mention_\\2', texts)\n",
    "  # demojize\n",
    "  texts = emoji.demojize(texts, delimiters=(\"\", \" \"))\n",
    "\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a410bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_df(tweets_file):\n",
    "   \n",
    "   tweets_data = pickle.load(open(tweets_file, 'rb'))\n",
    "\n",
    "  # Create Tweets DF\n",
    "   tweets_df = pd.DataFrame(columns = [\"ID\", \"Text\", \"Created at\", \n",
    "                                       \"Author ID\",\"Retweet Count\",\"Reply Count\",\n",
    "                                       \"Like Count\",\"Quote Count\",\"Lang\",\n",
    "                                       ],index=[-1])\n",
    "\n",
    "   for i in range(len(tweets_data)):\n",
    "      for twt in range(len(tweets_data[i]['data'])):\n",
    "         \n",
    "         twt_id = tweets_data[i]['data'][twt]['id']\n",
    "         text = tweets_data[i]['data'][twt]['text']\n",
    "         cleaned_text = process_text(text)\n",
    "         author_id=tweets_data[i]['data'][twt]['author_id']\n",
    "         created_at=tweets_data[i]['data'][twt]['created_at']\n",
    "         retweet_count=tweets_data[i]['data'][twt]['public_metrics']['retweet_count']\n",
    "         reply_count=tweets_data[i]['data'][twt]['public_metrics']['reply_count']\n",
    "         like_count=tweets_data[i]['data'][twt]['public_metrics']['like_count']\n",
    "         quote_count=tweets_data[i]['data'][twt]['public_metrics']['quote_count']\n",
    "         lang=tweets_data[i]['data'][twt]['lang']\n",
    "\n",
    "         #Create temp df to hold the new line\n",
    "         tmp_tweet_df = pd.DataFrame({ \"ID\": twt_id,\n",
    "                                 \"Text\": text,\n",
    "                                 \"Cleaned Text\": cleaned_text,\n",
    "                                 \"Author ID\": author_id,\n",
    "                                 \"Created at\": created_at,\n",
    "                                 \"Retweet Count\": retweet_count, \n",
    "                                 \"Reply Count\": reply_count,\n",
    "                                 \"Like Count\": like_count, \n",
    "                                 \"Quote Count\": quote_count,\n",
    "                                 \"Lang\": lang,\n",
    "                                  },index=[str(i)+str(twt)])\n",
    "         # Wrap text in quotes\n",
    "         tmp_tweet_df['Text'] = '\"' + tmp_tweet_df['Text'].astype(str) +'\"'\n",
    "         \n",
    "         # Concat the two dataframes\n",
    "         tweets_df = pd.concat([tweets_df, tmp_tweet_df],ignore_index=True)\n",
    "   return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17585b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../Models/Sentiment/twitter-roberta-latest/snapshots/13_11_22/ were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transformer pipeline with Roberta Sentimet Score\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "   \n",
    "tokenizer_ROBERTA = AutoTokenizer.from_pretrained(ROBERTA, \n",
    "                                                  local_files_only=True)\n",
    "config_ROBERTA = AutoConfig.from_pretrained(ROBERTA, \n",
    "                                                  local_files_only=True)\n",
    "model_roberta = AutoModelForSequenceClassification.from_pretrained(ROBERTA, \n",
    "                                                  local_files_only=True)\n",
    "\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_roberta, \n",
    "                                                tokenizer=tokenizer_ROBERTA,\n",
    "                                                device=device)\n",
    "@lox.thread(8)\n",
    "def roberta_sentiment(df):\n",
    "   # Clean Text again to ensure consistency\n",
    "   df['Cleaned Text']=df['Text'].apply(process_text)\n",
    "   #Get the Ouput as columns\n",
    "   df['Roberta Output']=df['Cleaned Text'].apply(sentiment_task)\n",
    "   df=df.explode('Roberta Output') #unnest the list\n",
    "   df=pd.concat([df.drop(['Roberta Output'], axis=1), df['Roberta Output'].apply(pd.Series)], axis=1)\n",
    "   df=df.rename(columns = {'label':'Sentiment Label','score':'Sentiment Score'})\n",
    "   \n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5636ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transformer pipeline with Stockwits Signal Scorer\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "   \n",
    "tokenizer_stockwits = RobertaTokenizer.from_pretrained(STOCKWITS,\n",
    "                                                       local_files_only=True)\n",
    "model_stockwits = RobertaForSequenceClassification.from_pretrained(STOCKWITS, \n",
    "                                                         local_files_only=True)\n",
    "\n",
    "stance_score = pipeline(\"text-classification\", model=model_stockwits, \n",
    "                                               tokenizer=tokenizer_stockwits,\n",
    "                                               device=device)\n",
    "@lox.thread(8)\n",
    "def stockwits_signal(df):\n",
    "   # Clean Text again to ensure consistency\n",
    "   df['Cleaned Text']=df['Text'].apply(process_text)\n",
    "   #Score and Get the Ouput as columns\n",
    "   df['Stockwits Output']=df['Cleaned Text'].apply(stance_score)   \n",
    "   df=df.explode('Stockwits Output') #unnest the list\n",
    "   df=pd.concat([df.drop(['Stockwits Output'], axis=1), df['Stockwits Output'].apply(pd.Series)], axis=1)\n",
    "   # 2 labels, label 0 is bearish, label 1 is bullish\n",
    "   df=df.replace({'LABEL_0':\"Bearish\", \"LABEL_1\": \"Bullish\"})\n",
    "   df=df.rename(columns = {'label':'Signal Label','score':'Signal Score'})\n",
    "\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ee42407",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path =\"../Data/RawTweets/\"\n",
    "#we shall store all the file names in this list\n",
    "raw_data_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(raw_path):\n",
    "\traw_data_files.extend(os.path.join(root,file) for file in files)\n",
    "raw_data_files = list(map(lambda st: str.replace(st, \"\\\\\", \"/\"), raw_data_files))\n",
    "\n",
    "# Replace RawTweets with  in list of strings\n",
    "cleaned_data_files = list(map(lambda st: str.replace(st, \"pkl\", \"csv\"), raw_data_files))\n",
    "cleaned_data_files = list(map(lambda st: str.replace(st, \"RawTweets\", \"CleanedTweets\"), cleaned_data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22f5e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if files are already converted and remove them from both lists\n",
    "#Find Already converted files\n",
    "scored_path =\"../Data/CleanedTweets/\"\n",
    "cleaned_files=[]\n",
    "ts = datetime.now()\n",
    "for root, dirs, files in os.walk(scored_path):\n",
    "\tcleaned_files.extend(os.path.join(root,file) for file in files)\n",
    "cleaned_files = list(map(lambda st: str.replace(st, \"\\\\\", \"/\"), cleaned_files))\n",
    "\n",
    "#Remove the already converted files\n",
    "with open('../Logs/Conversions.txt', 'a') as f:\n",
    "      f.write(f'{ts}-[Resume] - Resuming Conversion. \\n')\n",
    "      f.write(f'{ts}-[INFO] - Found Already {len(cleaned_files)} converted Files.\\n')\n",
    "        \n",
    "cleaned_data_files_set = set(cleaned_data_files)\n",
    "already_cleaned_set = set(cleaned_files)\n",
    "cleaned_data_files = list(cleaned_data_files_set - already_cleaned_set)\n",
    "raw_data_files = list(map(lambda st: str.replace(st, \"csv\",\"pkl\"), cleaned_data_files))\n",
    "raw_data_files = list(map(lambda st: str.replace(st, \"CleanedTweets\",\"RawTweets\"), raw_data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e315cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper Function for the Conversion to enable parrallelisation\n",
    "@lox.process(10)\n",
    "def conversion_scorrer_wrapper(i, raw_file):\n",
    "   ts = datetime.now()\n",
    "   with open('../Logs/Conversions.txt', 'a') as f:\n",
    "        f.write(f'{ts}-[INFO] - Converting tweets from file: {raw_file},\\n')\n",
    "        df=tweets_to_df(raw_file)\n",
    "        \n",
    "        f.write(f'{ts}-[INFO] - Scoring file: {raw_file}, with Stockwits and Roberta Models. \\n')\n",
    "        df=roberta_sentiment(df)\n",
    "        df=stockwits_signal(df)\n",
    "        ts = datetime.now()\n",
    "        f.write(f'{ts}-[INFO] - Conversion Done. Saving as csv to file: {cleaned_data_files[i]} \\n')\n",
    "        df.to_csv(cleaned_data_files[i], sep='~', encoding='utf-8')\n",
    "        f.write(f'{ts}-[INFO] - File saved. Files Remaining: {len(raw_data_files)-i} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f41ad67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003346350be0426990587c4cc4e831e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1914 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimta\\Anaconda3\\envs\\TwiterCuda\\lib\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#iterate over the raw files and clean and score them\n",
    "failed_rawfiles =[]\n",
    "for i, raw_file in tqdm_notebook(enumerate(raw_data_files), unit='files', total=len(raw_data_files)):\n",
    "   try:\n",
    "      conversion_scorrer_wrapper(i, raw_file)\n",
    "   except:\n",
    "          ts = datetime.now()\n",
    "          with open('../Logs/Conversions.txt', 'a') as f:\n",
    "            f.write(f'{ts}-[ERROR] - Failed to score file: {raw_file}. Adding it to list of raw files to be manual inspected. \\n')\n",
    "            failed_rawfiles.append(raw_file)\n",
    "          with open('../Logs/Conversion_Errors.txt', 'a') as f:\n",
    "            f.write(f'{ts}-[Failed] - Failed to score file: {raw_file}. Adding it to list of raw files to be manual inspected. \\n')  \n",
    "   time.sleep(0.01)\n",
    "   continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead5defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the csv's into Ticker and Ceo ones to create seperate Dataframes\n",
    "cleaned_tickers_path =\"../Data/CleanedTweets/Tickers\"\n",
    "#we shall store all the file names in this list\n",
    "cleaned_tickers_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(cleaned_tickers_path):\n",
    "\tcleaned_tickers_files.extend(os.path.join(root,file) for file in files)\n",
    "cleaned_tickers_files = list(map(lambda st: str.replace(st, \"\\\\\", \"/\"), cleaned_tickers_files))\n",
    "\n",
    "cleaned_ceos_path =\"../Data/CleanedTweets/Ceos\"\n",
    "cleaned_ceos_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(cleaned_ceos_path):\n",
    "\tcleaned_ceos_files.extend(os.path.join(root,file) for file in files)\n",
    "cleaned_ceos_files = list(map(lambda st: str.replace(st, \"\\\\\", \"/\"), cleaned_ceos_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a37e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tickers Dataframe\n",
    "tickers_dfs = []\n",
    "failed_tickers_dfs =[]\n",
    "ts = datetime.now()\n",
    "for i, csv_file in tqdm_notebook(enumerate(cleaned_tickers_files), unit='files', total=len(cleaned_tickers_files)):\n",
    "   with open('../Logs/CSV Processing.txt', 'a') as f:\n",
    "        f.write(f'{ts}-[INFO] - Processing tweets from csv file: {csv_file},\\n')\n",
    "        ts = datetime.now()\n",
    "        try:\n",
    "           data = pd.read_csv(csv_file, sep='~', encoding='utf-8', low_memory=False)\n",
    "           # Clean Text again to ensure consistency\n",
    "           data['Cleaned Text']=data['Text'].apply(process_text)\n",
    "\n",
    "           f.write(f'{ts}-[INFO] - Appending file: {csv_file}, to list of daraframes. \\n')\n",
    "           tickers_dfs.append(data)\n",
    "        except:\n",
    "           f.write(f'{ts}-[ERROR] - Failed to read file: {csv_file}. Adding it to list of daraframes to be redownloaded. \\n')\n",
    "           failed_tickers_dfs.append(csv_file)\n",
    "        ts = datetime.now()\n",
    "        f.write(f'{ts}-[INFO] - Files remaining: {len(cleaned_tickers_files)-i},\\n')\n",
    "tickers_df = pd.concat(tickers_dfs, ignore_index=True).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d304fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ceos Dataframe\n",
    "ceos_dfs = []\n",
    "failed_ceos_dfs =[]\n",
    "ts = datetime.now()\n",
    "for i, csv_file in tqdm_notebook(enumerate(cleaned_ceos_files), unit='files', total=len(cleaned_ceos_files)):\n",
    "   with open('../Logs/CSV Processing.txt', 'a') as f:\n",
    "        f.write(f'{ts}-[INFO] - Processing tweets from csv file: {csv_file},\\n')\n",
    "        ts = datetime.now()\n",
    "        try:\n",
    "           data = pd.read_csv(csv_file, sep='~', encoding='utf-8', low_memory=False)\n",
    "           # Clean Text again to ensure consistency\n",
    "           data['Cleaned Text']=data['Text'].apply(process_text)\n",
    "           f.write(f'{ts}-[INFO] - Appending file: {csv_file}, to list of daraframes. \\n')\n",
    "           ceos_dfs.append(data)\n",
    "        except:\n",
    "           f.write(f'{ts}-[ERROR] - Failed to read file: {csv_file}. Adding it to list of daraframes to be redownloaded. \\n')\n",
    "           failed_ceos_dfs.append(csv_file)\n",
    "        ts = datetime.now()\n",
    "        f.write(f'{ts}-[INFO] - Files remaining: {len(cleaned_ceos_files)-i},\\n')\n",
    "\n",
    "ceos_df = pd.concat(ceos_dfs, ignore_index=True).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_df.to_csv('../Data/ScoredDf/Tickers.csv', sep='~', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceos_df.to_csv('../Data/ScoredDf/Ceos.csv', sep='~', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd444aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed_ceos_dfs)+len(failed_tickers_dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df8a25b6",
   "metadata": {},
   "source": [
    "## Investigate missing values in Sentiment and Signal Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36109dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=ceos_df.head(100).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e363ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = roberta_sentiment(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8036ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TwiterCuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ff6407f33fc73a7ffcc68d3d5037606bda0033558bef3b98ee1cfabab1567283"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

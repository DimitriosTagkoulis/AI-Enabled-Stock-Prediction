{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2307c279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T18:52:49.840860Z",
     "start_time": "2022-08-09T18:52:49.789559Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import needed libraries\n",
    "import os\n",
    "import re\n",
    "import lox\n",
    "import time\n",
    "import emoji\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from datetime import datetime\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf3c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained models paths (Saved localy to save time for inference)\n",
    "STOCKWITS = '../Models/Sentiment/roberta-stocktwits-finetuned/snapshots/13_11_22/'\n",
    "ROBERTA = '../Models/Sentiment/twitter-roberta-latest/snapshots/13_11_22/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d4feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main text cleaning function\n",
    "def process_text(text):\n",
    "  text = str(text)\n",
    "  # make text lowercase\n",
    "  text = text.lower()\n",
    "  # remove URLs\n",
    "  text = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", \"\", text)\n",
    "  text = re.sub(r'https?://\\S+', \"\", text)\n",
    "  text = re.sub(r'www.\\S+', \"\", text)\n",
    "  text = re.sub(r'&[a-z]+;', \"\", text)\n",
    "  # Remove placeholders\n",
    "  text = re.sub(r'{link}', \"\", text)\n",
    "  text = re.sub(r\"\\[video\\]\", \"\", text)\n",
    "  # remove '\n",
    "  text = re.sub(r'{&#39;}', \"'\", text)\n",
    "  # remove new lines\n",
    "  text = re.sub(r'{\\n}', \" \", text)\n",
    "\n",
    "  text = text.strip()\n",
    "  # remove symbol names\n",
    "  text = re.sub(r'(\\#)(\\S+)', r'hashtag_\\2', text)\n",
    "  text = re.sub(r'(\\$)([A-Za-z]+)', r'cashtag_\\2', text)\n",
    "  # remove usernames\n",
    "  text = re.sub(r'(\\@)(\\S+)', r'mention_\\2', text)\n",
    "  # demojize\n",
    "  text = emoji.demojize(text, delimiters=(\"\", \" \"))\n",
    "  # remove substrings that occure multiple time if the text is big (handle emoji spam)\n",
    "  if len(text) > 500:\n",
    "    text = ' '.join(set(text.split()))\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a410bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter API response to df converter function\n",
    "def tweets_to_df(tweets_file):\n",
    "   \n",
    "   tweets_data = pickle.load(open(tweets_file, 'rb'))\n",
    "\n",
    "  # Create Tweets DF\n",
    "   tweets_df = pd.DataFrame(columns = [\"ID\", \"Text\", \"Created at\", \n",
    "                                       \"Author ID\",\"Retweet Count\",\"Reply Count\",\n",
    "                                       \"Like Count\",\"Quote Count\",\"Lang\",\n",
    "                                       ],index=[-1])\n",
    "   #Iterate over the tweets and add the individual fields to variables\n",
    "   for i in range(len(tweets_data)):\n",
    "      for twt in range(len(tweets_data[i]['data'])):\n",
    "         \n",
    "         twt_id = tweets_data[i]['data'][twt]['id']\n",
    "         text = tweets_data[i]['data'][twt]['text']\n",
    "         cleaned_text = process_text(text)\n",
    "         author_id=tweets_data[i]['data'][twt]['author_id']\n",
    "         created_at=tweets_data[i]['data'][twt]['created_at']\n",
    "         retweet_count=tweets_data[i]['data'][twt]['public_metrics']['retweet_count']\n",
    "         reply_count=tweets_data[i]['data'][twt]['public_metrics']['reply_count']\n",
    "         like_count=tweets_data[i]['data'][twt]['public_metrics']['like_count']\n",
    "         quote_count=tweets_data[i]['data'][twt]['public_metrics']['quote_count']\n",
    "         lang=tweets_data[i]['data'][twt]['lang']\n",
    "\n",
    "         #Create temp df to hold the new line\n",
    "         tmp_tweet_df = pd.DataFrame({ \"ID\": twt_id,\n",
    "                                 \"Text\": text,\n",
    "                                 \"Cleaned Text\": cleaned_text,\n",
    "                                 \"Author ID\": author_id,\n",
    "                                 \"Created at\": created_at,\n",
    "                                 \"Retweet Count\": retweet_count, \n",
    "                                 \"Reply Count\": reply_count,\n",
    "                                 \"Like Count\": like_count, \n",
    "                                 \"Quote Count\": quote_count,\n",
    "                                 \"Lang\": lang,\n",
    "                                  },index=[str(i)+str(twt)])\n",
    "         # Wrap text in quotes\n",
    "         tmp_tweet_df['Text'] = '\"' + tmp_tweet_df['Text'].astype(str) +'\"'\n",
    "         \n",
    "         # Concat the two dataframes\n",
    "         tweets_df = pd.concat([tweets_df, tmp_tweet_df],ignore_index=True)\n",
    "   return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17585b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer pipeline with Roberta Sentimet Score\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "   \n",
    "tokenizer_ROBERTA = AutoTokenizer.from_pretrained(ROBERTA, \n",
    "                                                  local_files_only=True)\n",
    "config_ROBERTA = AutoConfig.from_pretrained(ROBERTA, \n",
    "                                                  local_files_only=True)\n",
    "model_roberta = AutoModelForSequenceClassification.from_pretrained(ROBERTA, \n",
    "                                                  local_files_only=True)\n",
    "\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_roberta, \n",
    "                                                   tokenizer=tokenizer_ROBERTA,\n",
    "                                                   device=device)\n",
    "@lox.thread(8)\n",
    "def roberta_sentiment(df):\n",
    "   # Clean Text again to ensure consistency\n",
    "   df['Cleaned Text']=df['Text'].apply(process_text)\n",
    "   #Get the Ouput as columns\n",
    "   df['Roberta Output']=df['Cleaned Text'].apply(sentiment_task)\n",
    "   df=df.explode('Roberta Output') #unnest the list\n",
    "   df=pd.concat([df.drop(['Roberta Output'], axis=1), df['Roberta Output'].apply(pd.Series)], axis=1)\n",
    "   df=df.rename(columns = {'label':'Sentiment Label','score':'Sentiment Score'})\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5636ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer pipeline with Stockwits Signal Scorer\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "   \n",
    "tokenizer_stockwits = RobertaTokenizer.from_pretrained(STOCKWITS,\n",
    "                                                       local_files_only=True)\n",
    "model_stockwits = RobertaForSequenceClassification.from_pretrained(STOCKWITS, \n",
    "                                                         local_files_only=True)\n",
    "\n",
    "stance_score = pipeline(\"text-classification\", model=model_stockwits, \n",
    "                                                tokenizer=tokenizer_stockwits,\n",
    "                                                device=device)\n",
    "@lox.thread(8)\n",
    "def stockwits_signal(df):\n",
    "   # Clean Text again to ensure consistency\n",
    "   df['Cleaned Text']=df['Text'].apply(process_text)\n",
    "   #Score and Get the Ouput as columns\n",
    "   df['Stockwits Output']=df['Cleaned Text'].apply(stance_score)\n",
    "   df=df.explode('Stockwits Output') #unnest the list\n",
    "   df=pd.concat([df.drop(['Stockwits Output'], axis=1), df['Stockwits Output'].apply(pd.Series)], axis=1)\n",
    "   # 2 labels, label 0 is bearish, label 1 is bullish\n",
    "   df=df.replace({'LABEL_0':\"Bearish\", \"LABEL_1\": \"Bullish\"})\n",
    "   df=df.rename(columns = {'label':'Signal Label','score':'Signal Score'})\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ee42407",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path =\"../Data/RawTweets/\"\n",
    "#we shall store all the file names in this list\n",
    "raw_data_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(raw_path):\n",
    "\traw_data_files.extend(os.path.join(root,file) for file in files)\n",
    "raw_data_files = list(map(lambda st: str.replace(st, \"\\\\\", \"/\"), raw_data_files))\n",
    "\n",
    "# Replace RawTweets with  in list of strings\n",
    "cleaned_data_files = list(map(lambda st: str.replace(st, \"pkl\", \"csv\"), raw_data_files))\n",
    "cleaned_data_files = list(map(lambda st: str.replace(st, \"RawTweets\", \"CleanedTweets\"), cleaned_data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22f5e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if files are already converted and remove them from both lists\n",
    "#Find Already converted files\n",
    "scored_path =\"../Data/CleanedTweets/\"\n",
    "cleaned_files=[]\n",
    "ts = datetime.now()\n",
    "for root, dirs, files in os.walk(scored_path):\n",
    "\tcleaned_files.extend(os.path.join(root,file) for file in files)\n",
    "cleaned_files = list(map(lambda st: str.replace(st, \"\\\\\", \"/\"), cleaned_files))\n",
    "\n",
    "#Remove the already converted files\n",
    "with open('../Logs/Conversions.txt', 'a') as f:\n",
    "      f.write(f'{ts}-[Resume] - Resuming Conversion. \\n')\n",
    "      f.write(f'{ts}-[INFO] - Found Already {len(cleaned_files)} converted Files.\\n')\n",
    "        \n",
    "cleaned_data_files_set = set(cleaned_data_files)\n",
    "already_cleaned_set = set(cleaned_files)\n",
    "cleaned_data_files = list(cleaned_data_files_set - already_cleaned_set)\n",
    "raw_data_files = list(map(lambda st: str.replace(st, \"csv\",\"pkl\"), cleaned_data_files))\n",
    "raw_data_files = list(map(lambda st: str.replace(st, \"CleanedTweets\",\"RawTweets\"), raw_data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e315cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper Function for the Conversion to enable parrallelisation\n",
    "@lox.process(10)\n",
    "def conversion_scorrer_wrapper(i, raw_file):\n",
    "   ts = datetime.now()\n",
    "   with open('../Logs/Conversions.txt', 'a') as f:\n",
    "        f.write(f'{ts}-[INFO] - Converting tweets from file: {raw_file},\\n')\n",
    "        #Call df conversion function\n",
    "        df=tweets_to_df(raw_file)\n",
    "        f.write(f'{ts}-[INFO] - Scoring file: {raw_file}, with Stockwits and Roberta Models. \\n')\n",
    "        #Call scoring functions\n",
    "        df=roberta_sentiment(df)\n",
    "        df=stockwits_signal(df)\n",
    "        ts = datetime.now()\n",
    "        f.write(f'{ts}-[INFO] - Conversion Done. Saving as csv to file: {cleaned_data_files[i]} \\n')\n",
    "        df.to_csv(cleaned_data_files[i], sep='~', encoding='utf-8')\n",
    "        f.write(f'{ts}-[INFO] - File saved. Files Remaining: {len(raw_data_files)-i} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ad67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the raw files and clean and score them\n",
    "failed_rawfiles=[]\n",
    "for i, raw_file in tqdm_notebook(enumerate(raw_data_files), unit='files', total=len(raw_data_files)):\n",
    "   try:\n",
    "      conversion_scorrer_wrapper(i, raw_file)\n",
    "   except:\n",
    "          ts = datetime.now()\n",
    "          with open('../Logs/Conversions.txt', 'a') as f:\n",
    "            f.write(f'{ts}-[ERROR] - Failed to score file: {raw_file}. Adding it to list of raw files to be manual inspected. \\n')\n",
    "            failed_rawfiles.append(raw_file)\n",
    "          with open('../Logs/Conversion_Errors.txt', 'a') as f:\n",
    "            f.write(f'{ts}-[Failed] - Failed to score file: {raw_file}. Adding it to list of raw files to be manual inspected. \\n')  \n",
    "   time.sleep(0.01)\n",
    "   continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e2b55e5",
   "metadata": {},
   "source": [
    "## Create Final Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ead5defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the csv's into Ticker and Ceo ones to create seperate Dataframes\n",
    "cleaned_tickers_path =\"../Data/CleanedTweets/Tickers\"\n",
    "#we shall store all the file names in this list\n",
    "cleaned_tickers_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(cleaned_tickers_path):\n",
    "\tcleaned_tickers_files.extend(os.path.join(root,file) for file in files)\n",
    "cleaned_tickers_files = list(map(lambda st: str.replace(st, \"\\\\\", \"/\"), cleaned_tickers_files))\n",
    "\n",
    "cleaned_ceos_path =\"../Data/CleanedTweets/Ceos\"\n",
    "cleaned_ceos_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(cleaned_ceos_path):\n",
    "\tcleaned_ceos_files.extend(os.path.join(root,file) for file in files)\n",
    "cleaned_ceos_files = list(map(lambda st: str.replace(st, \"\\\\\", \"/\"), cleaned_ceos_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4a37e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037a3a5d80c9467b93eedf3187a210c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3288 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Tickers Dataframe\n",
    "tickers_dfs = []\n",
    "failed_tickers_dfs =[]\n",
    "ts = datetime.now()\n",
    "for i, csv_file in tqdm_notebook(enumerate(cleaned_tickers_files), unit='files', total=len(cleaned_tickers_files)):\n",
    "   with open('../Logs/CSV Processing.txt', 'a') as f:\n",
    "        f.write(f'{ts}-[INFO] - Processing tweets from csv file: {csv_file},\\n')\n",
    "        ts = datetime.now()\n",
    "        try:\n",
    "           data = pd.read_csv(csv_file, sep='~', encoding='utf-8', low_memory=False)\n",
    "           # Clean Text again to ensure consistency\n",
    "           data['Cleaned Text']=data['Text'].apply(process_text)\n",
    "           data['File']=csv_file\n",
    "           f.write(f'{ts}-[INFO] - Appending file: {csv_file}, to list of daraframes. \\n')\n",
    "           tickers_dfs.append(data)\n",
    "        except:\n",
    "           f.write(f'{ts}-[ERROR] - Failed to read file: {csv_file}. Adding it to list of daraframes to be redownloaded. \\n')\n",
    "           failed_tickers_dfs.append(csv_file)\n",
    "        ts = datetime.now()\n",
    "        f.write(f'{ts}-[INFO] - Files remaining: {len(cleaned_tickers_files)-i},\\n')\n",
    "tickers_df = pd.concat(tickers_dfs, ignore_index=True).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17d304fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790a3f8dcf024964bb0ab5f8efd0fb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3288 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Ceos Dataframe\n",
    "ceos_dfs = []\n",
    "failed_ceos_dfs =[]\n",
    "ts = datetime.now()\n",
    "for i, csv_file in tqdm_notebook(enumerate(cleaned_ceos_files), unit='files', total=len(cleaned_ceos_files)):\n",
    "   with open('../Logs/CSV Processing.txt', 'a') as f:\n",
    "        f.write(f'{ts}-[INFO] - Processing tweets from csv file: {csv_file},\\n')\n",
    "        ts = datetime.now()\n",
    "        try:\n",
    "           data = pd.read_csv(csv_file, sep='~', encoding='utf-8', low_memory=False)\n",
    "           # Clean Text again to ensure consistency\n",
    "           data['Cleaned Text']=data['Text'].apply(process_text)\n",
    "           data['File'] = csv_file\n",
    "           f.write(f'{ts}-[INFO] - Appending file: {csv_file}, to list of daraframes. \\n')\n",
    "           ceos_dfs.append(data)\n",
    "        except:\n",
    "           f.write(f'{ts}-[ERROR] - Failed to read file: {csv_file}. Adding it to list of daraframes to be redownloaded. \\n')\n",
    "           failed_ceos_dfs.append(csv_file)\n",
    "        ts = datetime.now()\n",
    "        f.write(f'{ts}-[INFO] - Files remaining: {len(cleaned_ceos_files)-i},\\n')\n",
    "\n",
    "ceos_df = pd.concat(ceos_dfs, ignore_index=True).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f4146b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only the id dates labels and file before saving\n",
    "tickers_df=tickers_df[[\"ID\",\"Created at\",\"Sentiment Label\", \"Sentiment Score\", \"Signal Label\", \"Signal Score\", \"File\"]]\n",
    "ceos_df=ceos_df[[\"ID\",\"Created at\",\"Sentiment Label\", \"Sentiment Score\", \"Signal Label\", \"Signal Score\", \"File\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a019a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the files\n",
    "tickers_df.to_csv('../Data/ScoredDf/Tickers.csv', sep='~', encoding='utf-8')\n",
    "ceos_df.to_csv('../Data/ScoredDf/Ceos.csv', sep='~', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TwitterCuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1b6b6c7e0270ec7b35f7435c5c115be2ae49da16c33c029292df293af6bafc80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

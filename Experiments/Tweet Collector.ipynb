{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2307c279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T18:52:49.840860Z",
     "start_time": "2022-08-09T18:52:49.789559Z"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from searchtweets import ResultStream, gen_request_parameters, load_credentials, read_config\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pickle\n",
    "import argparse\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32844c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define date range function\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "# Save pickle file function\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'ab') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e923b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Daily Tweet Counts\n",
    "def get_tweet_counts(query,start,end):\n",
    "   counts = client.get_all_tweets_count(query=query,end_time=end,\n",
    "                                        start_time=start, granularity='day')\n",
    "   daily_counts = pd.DataFrame(columns = [\"Day\", \"Tweet Volume\"])\n",
    "   for count in counts.data:\n",
    "      c = pd.DataFrame({\"Day\" : count[\"start\"],\n",
    "                        \"Tweet Volume\" : count[\"tweet_count\"]},index=[0])\n",
    "      daily_counts = pd.concat([daily_counts, c] , ignore_index=True)\n",
    "   return daily_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d815bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(query, prefix, start, end):\n",
    "    for single_date in daterange(start, end):\n",
    "         \n",
    "      # set start timestamp\n",
    "      start_ts = single_date\n",
    "\n",
    "      # set end timestamp\n",
    "      end_ts =  single_date + timedelta(days=1)\n",
    "\n",
    "      # payload rules for v2 api\n",
    "      rule = gen_request_parameters(query = query,\n",
    "                                    results_per_call = 500,\n",
    "                                    start_time = start_ts.isoformat(),\n",
    "                                    end_time = end_ts.isoformat(),\n",
    "                                    tweet_fields = tweet_fields,\n",
    "                                    user_fields = user_fields,\n",
    "                                    expansions = 'author_id',\n",
    "                                    granularity = None,\n",
    "                                    stringify = False)\n",
    "\n",
    "      # result stream from twitter v2 api\n",
    "      rs = ResultStream(request_parameters = rule,\n",
    "                        max_results = 1000000,\n",
    "                        max_pages = 1,\n",
    "                        max_tweets = 10000,\n",
    "                        **search_creds)\n",
    "\n",
    "      # number of reconnection tries\n",
    "      tries = 10\n",
    "\n",
    "      while True:\n",
    "         tries -= 1\n",
    "         try:\n",
    "                  # indicate which day is getting retrieved\n",
    "               print(f'[INFO] - Retrieving tweets from {str(start_ts)}, for querry: {query}')\n",
    "               with open('../Logs/Queries.txt', 'a') as f:\n",
    "                    f.write(f'[INFO] - Retrieving tweets from {str(start_ts)}, for querry: {query} \\n')\n",
    "\n",
    "               # get json response to list\n",
    "               tweets = list(rs.stream())\n",
    "\n",
    "               # break free from while loop\n",
    "               break\n",
    "         except Exception as err:\n",
    "               if tries == 0:\n",
    "                  raise err\n",
    "               print(f'[INFO] - Got connection error, waiting 15 seconds and trying again. {tries} tries left.')\n",
    "               with open('../Logs/Queries.txt', 'a') as f:\n",
    "                    f.write(f'[INFO] - Got connection error, waiting 15 seconds and trying again. {tries} tries left. \\n')\n",
    "\n",
    "               time.sleep(15)\n",
    "\n",
    "      # parse results to dataframe\n",
    "      print(f'[INFO] - Saving tweets from {str(start_ts)}')\n",
    "      with open('../Logs/Queries.txt', 'a') as f:\n",
    "           f.write(f'[INFO] - Saving tweets from {str(start_ts)} \\n')\n",
    "      file_prefix_w_date = prefix + start_ts.isoformat()\n",
    "      outpickle = f'{file_prefix_w_date}.pkl'\n",
    "      save_object(tweets, outpickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9245081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the search parameters\n",
    "date_start = \"2019-01-01\"\n",
    "date_end = \"2022-01-01\"\n",
    "user_fields = \",\".join(['public_metrics'])\n",
    "tweet_fields = \",\".join(['created_at','geo','id','lang', 'public_metrics',\n",
    "                        'source','text'])\n",
    "\n",
    "# get waittime\n",
    "waittime = 45\n",
    "\n",
    "# load twitter keys\n",
    "search_creds = load_credentials('.twitter_keys.yaml',\n",
    "                                 yaml_key = 'search_tweets_v2',\n",
    "                                 env_overwrite = False)\n",
    "\n",
    "# set interval to loop through\n",
    "start_date = datetime.strptime(date_start, '%Y-%m-%d').date()\n",
    "end_date = datetime.strptime(date_end, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a59b5cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionairy for queries\n",
    "companies = [\"Apple\", \"Amazon\", \"Tesla\"]\n",
    "ceos = [\"Tim Cook\", \"Jeff Bezos\",\"Elon Musk\"]\n",
    "tickers = [\"AAPL\", \"AMZN\", \"TSLA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a4ea91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Retrieving tweets from 2021-12-31, for querry: AAPL -is:retweet -is:reply lang:en\n",
      "[INFO] - Saving tweets from 2021-12-31\n",
      "[INFO] - Retrieving tweets from 2021-12-31, for querry: AMZN -is:retweet -is:reply lang:en\n",
      "[INFO] - Saving tweets from 2021-12-31\n",
      "[INFO] - Retrieving tweets from 2021-12-31, for querry: TSLA -is:retweet -is:reply lang:en\n",
      "[INFO] - Saving tweets from 2021-12-31\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    search_words = f\"{ticker} -is:retweet -is:reply lang:en\"\n",
    "    filename_prefix = f\"../Data/RawTweets/Tickers/{ticker}/\"\n",
    "    \n",
    "    get_tweets(search_words, filename_prefix, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62023b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Retrieving tweets from 2021-12-31, for querry: Tim Cook -is:retweet -is:reply lang:en\n",
      "[INFO] - Saving tweets from 2021-12-31\n",
      "[INFO] - Retrieving tweets from 2021-12-31, for querry: Jeff Bezos -is:retweet -is:reply lang:en\n",
      "[INFO] - Saving tweets from 2021-12-31\n",
      "[INFO] - Retrieving tweets from 2021-12-31, for querry: Elon Musk -is:retweet -is:reply lang:en\n",
      "[INFO] - Saving tweets from 2021-12-31\n"
     ]
    }
   ],
   "source": [
    "for ceo in ceos:\n",
    "    search_words = f\"{ceo} -is:retweet -is:reply lang:en\"\n",
    "    filename_prefix = f\"../Data/RawTweets/Ceos/{ceo}/\"\n",
    "    \n",
    "    get_tweets(search_words, filename_prefix, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for company in companies:\n",
    "    search_words = f\"{company} -is:retweet -is:reply lang:en\"\n",
    "    filename_prefix = f\"../Data/RawTweets/Companies/{company}/\"\n",
    "    \n",
    "    get_tweets(search_words, filename_prefix, start_date, end_date)\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "a2aa6dc29ecdc789877fdeba5516f26a01b54273e1637c69be7329f154fbc8c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
